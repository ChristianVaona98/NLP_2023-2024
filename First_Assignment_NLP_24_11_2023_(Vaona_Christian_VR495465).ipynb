{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia Text Classifier\n",
        "\n",
        "## Overview\n",
        "This Python script utilizes the Wikipedia API and the Natural Language Toolkit (NLTK) library to fetch, preprocess, and classify articles into medical and non-medical categories. The classification is performed using the Naive Bayes classifier from NLTK and preprocess the text before creating features using a Bag of Words approach. In particular the extract_features function utilizes the NLTK library to create a frequency distribution of the preprocessed words for each text.\n",
        "\n",
        "## Technology Used\n",
        "- **Programming Language:** Python\n",
        "- **Libraries:**\n",
        "  - `wikipediaapi`: Used to interact with the Wikipedia API.\n",
        "  - `nltk`: The Natural Language Toolkit for natural language processing tasks.\n",
        "  \n",
        "## Functions\n",
        "\n",
        "### `get_wikipedia_content(title)`\n",
        "- **Description:** Fetches the content of a Wikipedia page with the given title.\n",
        "- **Parameters:**\n",
        "  - `title` (str): The title of the Wikipedia page.\n",
        "- **Returns:** The text content of the Wikipedia page or `None` if the page does not exist.\n",
        "\n",
        "### `get_category_members(category, num_pages)`\n",
        "- **Description:** Fetches a specified number of articles from a given Wikipedia category.\n",
        "- **Parameters:**\n",
        "  - `category` (str): The name of the Wikipedia category.\n",
        "  - `num_pages` (int): The number of articles to fetch.\n",
        "- **Returns:** A list of article contents from the specified category.\n",
        "\n",
        "### `preprocess_text(text)`\n",
        "- **Description:** Preprocesses text by tokenizing, converting to lowercase, and removing stop words.\n",
        "- **Parameters:**\n",
        "  - `text` (str): The input text to be preprocessed.\n",
        "- **Returns:** A list of preprocessed words.\n",
        "\n",
        "### `extract_features(annotated_texts, category)`\n",
        "- **Description:** Extracts features from annotated texts by creating frequency distributions of preprocessed words.\n",
        "- **Parameters:**\n",
        "  - `annotated_texts` (list): A list of annotated texts.\n",
        "  - `category` (str): The category label for the texts.\n",
        "- **Returns:** A list of features, where each feature is a tuple containing a frequency distribution and the category label.\n",
        "\n",
        "### `split_dataset(features)`\n",
        "- **Description:** Splits the dataset into training and testing sets.\n",
        "- **Parameters:**\n",
        "  - `features` (list): The list of features to be split.\n",
        "- **Returns:** Two lists representing the training set and testing set.\n",
        "\n",
        "## Main Script (`__main__` block)\n",
        "1. **Fetch Articles:**\n",
        "   - Fetches a specified number of medical and non-medical articles from Wikipedia for selected categories.\n",
        "   - Categories: Medicine, Cardiology, Neurology, Oncology, History, Building, Geography, Mathematics.\n",
        "\n",
        "2. **Preprocess Texts:**\n",
        "   - Preprocesses the fetched texts by removing stop words.\n",
        "\n",
        "3. **Extract Features:**\n",
        "   - Creates features for medical and non-medical texts using frequency distributions of preprocessed words.\n",
        "\n",
        "4. **Combine and Shuffle:**\n",
        "   - Combines medical and non-medical features into a single dataset.\n",
        "   - Shuffles the dataset to ensure randomness.\n",
        "\n",
        "5. **Train and Test Classifier:**\n",
        "   - Splits the dataset into training and testing sets.\n",
        "   - Trains a Naive Bayes classifier using the training set.\n",
        "   - Tests the classifier using the testing set and prints the accuracy.\n",
        "\n",
        "6. **Save Classifier:**\n",
        "   - Saves the trained classifier to a file (`naive_bayes_classifier.pickle`) for future use.\n",
        "\n",
        "## Note\n",
        "- Adjust the `num_medical_articles` and `num_non_medical_articles` variables to control the number of articles fetched for each category.\n",
        "- The script introduces a delay (`time.sleep(1)`) between API requests to avoid exceeding rate limits.\n",
        "\n",
        "## Dependencies\n",
        "- Ensure you have the required libraries installed by running:\n",
        "  ```bash\n",
        "  pip install wikipedia-api nltk\n",
        "  ```"
      ],
      "metadata": {
        "id": "zaFkJAm4bxWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FGKfY4DoZhL",
        "outputId": "bf7da93a-b1d5-4715-9e1c-68feaf5eeef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2023.7.22)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Set your own user agent\n",
        "USER_AGENT = \"WikipediaNLTK_Classifier/1.0\"\n",
        "\n",
        "def get_wikipedia_content(title):\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': USER_AGENT})\n",
        "    page_py = wiki_wiki.page(title)\n",
        "\n",
        "    if page_py.exists():\n",
        "        return page_py.text\n",
        "    else:\n",
        "        print(f\"Page '{title}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "def get_category_members(category, num_pages):\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': 'your-user-agent'})\n",
        "    category_page = wiki_wiki.page(f\"Category:{category}\")\n",
        "\n",
        "    members = list(category_page.categorymembers.values())\n",
        "    selected_members = members[:num_pages]\n",
        "\n",
        "    content_list = []\n",
        "    for member in selected_members:\n",
        "        title = member.title\n",
        "        content = get_wikipedia_content(title)\n",
        "        if content:\n",
        "            content_list.append(content)\n",
        "\n",
        "    return content_list\n",
        "\n",
        "# Function to preprocess text by removing stop words\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# Function to extract features from annotated texts\n",
        "def extract_features(annotated_texts, category):\n",
        "    features = []\n",
        "    for text in annotated_texts:\n",
        "        preprocessed_text = preprocess_text(text)\n",
        "        features.append((FreqDist(preprocessed_text), category))\n",
        "    return features\n",
        "\n",
        "# Function to split the dataset into training and testing sets\n",
        "def split_dataset(features):\n",
        "    split_index = int(0.8 * len(features))\n",
        "    training_set = features[:split_index]\n",
        "    testing_set = features[split_index:]\n",
        "    return training_set, testing_set\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fetch n number of medical and non-medical articles in this case 100 for each category\n",
        "    num_medical_articles = 100\n",
        "    num_non_medical_articles = 100\n",
        "    # If the total number is less than that indicated it may happen that the articles in that category are fewer than those requested\n",
        "\n",
        "    medical_categories = ['Medicine','Cardiology', 'Neurology', 'Oncology']\n",
        "    non_medical_categories = ['History', 'Building', 'Geography', 'Mathematics']\n",
        "\n",
        "    medical_texts = []\n",
        "    num_articles_in_category = 0\n",
        "    for category in medical_categories:\n",
        "        medical_texts.extend(get_category_members(category, num_pages=num_medical_articles))\n",
        "        num_articles_in_category = len(get_category_members(category, num_pages=num_medical_articles))\n",
        "        print(f\"Number of {category} Articles Fetched: {num_articles_in_category}\")\n",
        "        time.sleep(1)  # Introduce a delay between API requests\n",
        "\n",
        "    non_medical_texts = []\n",
        "    num_articles_in_category = 0\n",
        "    for category in non_medical_categories:\n",
        "        non_medical_texts.extend(get_category_members(category, num_pages=num_non_medical_articles))\n",
        "        num_articles_in_category = len(get_category_members(category, num_pages=num_medical_articles))\n",
        "        print(f\"Number of {category} Articles Fetched: {num_articles_in_category}\")\n",
        "        time.sleep(1)  # Introduce a delay between API requests\n",
        "\n",
        "    '''\n",
        "    ONLY FOR DEBUG\n",
        "\n",
        "    # Print the content of fetched articles\n",
        "    print(\"Medical Articles:\")\n",
        "    for i, text in enumerate(medical_texts):\n",
        "        print(f\"{i + 1}. {text[:50]}...\")\n",
        "\n",
        "    print(\"\\nNon-Medical Articles:\")\n",
        "    for i, text in enumerate(non_medical_texts):\n",
        "        print(f\"{i + 1}. {text[:50]}...\")\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Extract features from annotated texts\n",
        "    medical_features = extract_features(medical_texts, 'medical')\n",
        "    non_medical_features = extract_features(non_medical_texts, 'non-medical')\n",
        "\n",
        "    # Combine features and shuffle the dataset\n",
        "    all_features = medical_features + non_medical_features\n",
        "\n",
        "    # Print debug information\n",
        "    print(f\"\\nNumber of medical features: {len(medical_features)}\")\n",
        "    print(f\"Number of non-medical features: {len(non_medical_features)}\")\n",
        "    print(f\"Total number of features: {len(all_features)}\")\n",
        "\n",
        "    # Check if there are features before shuffling\n",
        "    if all_features:\n",
        "        random.shuffle(all_features)\n",
        "\n",
        "        # Split the dataset into training and testing sets\n",
        "        training_set, testing_set = split_dataset(all_features)\n",
        "\n",
        "        # Check if the training set has data\n",
        "        if training_set:\n",
        "            # Train the Naive Bayes classifier\n",
        "            classifier = NaiveBayesClassifier.train(training_set)\n",
        "\n",
        "            # Test the classifier\n",
        "            accuracy = nltk.classify.accuracy(classifier, testing_set)\n",
        "            print(f'Accuracy: {accuracy}')\n",
        "\n",
        "            # Save the classifier to a file for future use\n",
        "            with open('naive_bayes_classifier.pickle', 'wb') as file:\n",
        "                pickle.dump(classifier, file)\n",
        "        else:\n",
        "            print(\"Training set is empty. Check your data.\")\n",
        "    else:\n",
        "        print(\"No features extracted. Check your data.\")"
      ],
      "metadata": {
        "id": "uvUBLzAgoA-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7801a0-4635-4cbe-ab48-6aa15860358c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Medicine Articles Fetched: 14\n",
            "Number of Cardiology Articles Fetched: 100\n",
            "Number of Neurology Articles Fetched: 100\n",
            "Number of Oncology Articles Fetched: 100\n",
            "Number of History Articles Fetched: 35\n",
            "Number of Building Articles Fetched: 88\n",
            "Number of Geography Articles Fetched: 60\n",
            "Number of Mathematics Articles Fetched: 16\n",
            "\n",
            "Number of medical features: 314\n",
            "Number of non-medical features: 199\n",
            "Total number of features: 513\n",
            "Accuracy: 0.9223300970873787\n"
          ]
        }
      ]
    }
  ]
}