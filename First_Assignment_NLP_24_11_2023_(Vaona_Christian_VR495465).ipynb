{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FGKfY4DoZhL",
        "outputId": "bf7da93a-b1d5-4715-9e1c-68feaf5eeef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2023.7.22)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Set your own user agent\n",
        "USER_AGENT = \"WikipediaNLTK_Classifier/1.0\"\n",
        "\n",
        "def get_wikipedia_content(title):\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': USER_AGENT})\n",
        "    page_py = wiki_wiki.page(title)\n",
        "\n",
        "    if page_py.exists():\n",
        "        return page_py.text\n",
        "    else:\n",
        "        print(f\"Page '{title}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "def get_category_members(category, num_pages):\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': 'your-user-agent'})\n",
        "    category_page = wiki_wiki.page(f\"Category:{category}\")\n",
        "\n",
        "    members = list(category_page.categorymembers.values())\n",
        "    selected_members = members[:num_pages]\n",
        "\n",
        "    content_list = []\n",
        "    for member in selected_members:\n",
        "        title = member.title\n",
        "        content = get_wikipedia_content(title)\n",
        "        if content:\n",
        "            content_list.append(content)\n",
        "\n",
        "    return content_list\n",
        "\n",
        "# Function to preprocess text by removing stop words\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# Function to extract features from annotated texts\n",
        "def extract_features(annotated_texts, category):\n",
        "    features = []\n",
        "    for text in annotated_texts:\n",
        "        preprocessed_text = preprocess_text(text)\n",
        "        features.append((FreqDist(preprocessed_text), category))\n",
        "    return features\n",
        "\n",
        "# Function to split the dataset into training and testing sets\n",
        "def split_dataset(features):\n",
        "    split_index = int(0.8 * len(features))\n",
        "    training_set = features[:split_index]\n",
        "    testing_set = features[split_index:]\n",
        "    return training_set, testing_set\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fetch n number of medical and non-medical articles in this case 100 for each category\n",
        "    num_medical_articles = 100\n",
        "    num_non_medical_articles = 100\n",
        "    # If the total number is less than that indicated it may happen that the articles in that category are fewer than those requested\n",
        "\n",
        "    medical_categories = ['Medicine','Cardiology', 'Neurology', 'Oncology']\n",
        "    non_medical_categories = ['History', 'Building', 'Geography', 'Mathematics']\n",
        "\n",
        "    medical_texts = []\n",
        "    num_articles_in_category = 0\n",
        "    for category in medical_categories:\n",
        "        medical_texts.extend(get_category_members(category, num_pages=num_medical_articles))\n",
        "        num_articles_in_category = len(get_category_members(category, num_pages=num_medical_articles))\n",
        "        print(f\"Number of {category} Articles Fetched: {num_articles_in_category}\")\n",
        "        time.sleep(1)  # Introduce a delay between API requests\n",
        "\n",
        "    non_medical_texts = []\n",
        "    num_articles_in_category = 0\n",
        "    for category in non_medical_categories:\n",
        "        non_medical_texts.extend(get_category_members(category, num_pages=num_non_medical_articles))\n",
        "        num_articles_in_category = len(get_category_members(category, num_pages=num_medical_articles))\n",
        "        print(f\"Number of {category} Articles Fetched: {num_articles_in_category}\")\n",
        "        time.sleep(1)  # Introduce a delay between API requests\n",
        "\n",
        "    '''\n",
        "    ONLY FOR DEBUG\n",
        "\n",
        "    # Print the content of fetched articles\n",
        "    print(\"Medical Articles:\")\n",
        "    for i, text in enumerate(medical_texts):\n",
        "        print(f\"{i + 1}. {text[:50]}...\")\n",
        "\n",
        "    print(\"\\nNon-Medical Articles:\")\n",
        "    for i, text in enumerate(non_medical_texts):\n",
        "        print(f\"{i + 1}. {text[:50]}...\")\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Extract features from annotated texts\n",
        "    medical_features = extract_features(medical_texts, 'medical')\n",
        "    non_medical_features = extract_features(non_medical_texts, 'non-medical')\n",
        "\n",
        "    # Combine features and shuffle the dataset\n",
        "    all_features = medical_features + non_medical_features\n",
        "\n",
        "    # Print debug information\n",
        "    print(f\"\\nNumber of medical features: {len(medical_features)}\")\n",
        "    print(f\"Number of non-medical features: {len(non_medical_features)}\")\n",
        "    print(f\"Total number of features: {len(all_features)}\")\n",
        "\n",
        "    # Check if there are features before shuffling\n",
        "    if all_features:\n",
        "        random.shuffle(all_features)\n",
        "\n",
        "        # Split the dataset into training and testing sets\n",
        "        training_set, testing_set = split_dataset(all_features)\n",
        "\n",
        "        # Check if the training set has data\n",
        "        if training_set:\n",
        "            # Train the Naive Bayes classifier\n",
        "            classifier = NaiveBayesClassifier.train(training_set)\n",
        "\n",
        "            # Test the classifier\n",
        "            accuracy = nltk.classify.accuracy(classifier, testing_set)\n",
        "            print(f'Accuracy: {accuracy}')\n",
        "\n",
        "            # Save the classifier to a file for future use\n",
        "            with open('naive_bayes_classifier.pickle', 'wb') as file:\n",
        "                pickle.dump(classifier, file)\n",
        "        else:\n",
        "            print(\"Training set is empty. Check your data.\")\n",
        "    else:\n",
        "        print(\"No features extracted. Check your data.\")"
      ],
      "metadata": {
        "id": "uvUBLzAgoA-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7801a0-4635-4cbe-ab48-6aa15860358c"
      },
      "execution_count": null,
      "outputs": [
      ]
    }
  ]
}
